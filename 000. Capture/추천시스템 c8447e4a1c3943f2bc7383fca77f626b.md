# 추천시스템

생성일: 2022년 5월 17일 오전 9:55

[점프 투 파이썬](https://wikidocs.net/24603)


# 콘텐츠기반 필터링 추천시스템

사용자가 특정아이템을 매우 선호하는 경우

그 아이템과 비슷한 컨텐츠를 가진 다른 아이템을 추천하는 방식

콘텐츠 자체 정보를 바탕으로 유사도 분석

사용자 정보없이 보유한 데이터만으로  추천가능

필요한 정보는 적지만 범위에 있어 훨씬 제한적임

### 콘텐츠 기반 필터링 구현 프로세스

(1) 콘텐츠에 대한 여러 텍스트 정보들을 피처 벡터화

(2) 코사인 유사도로 **컨텐츠별** 유사도 계산

(3) 컨텐츠 별로 **가중 평점** 계산

(4) 유사도가 높은 컨텐츠 중에 평점이 좋은 컨텐츠 순으로 추천

### 피처 벡터화

- 텍스트를 특정 의미를 가지는 숫자형 값인 벡터 값으로 변환하는 것
    - (머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작)
- 텍스트 등의 데이터는 머신러닝 알고리즘에 바로 입력 불가하기 때문
- 텍스트를 단어로 추출해 피처로 할당하고 각 단어의 발생 빈도와 같은 값을 피처에 부여해서 
단어 피처의 발생 빈도 값으로 구성된 벡터로 만드는 기법
- 피처 벡터화는 기존 텍스트 데이터를 또 다른 형태의 피처의 조합으로 변경하기 때문에 
넓은 의미의 피처 추출에 포함

### **피처 벡터화 방식**

- 카운트 기반의 벡터화
- TF-IDF(Term Frequency-Inverse Document Frequency) 기반의 벡터화

### **카운트 기반의 벡터화**

- 단어 피처에 값을 부여할 때 단어의 빈도 수, 즉 Count를 부여하는 것
- 카운트 값이 높을수록 중요 단어로 인식
- 카운트만 부여할 경우 그 단어의 특징을 나타내기 보다는 
언어의 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하게 됨
- 보완 : TF-IDF 벡터화

### TF-IDF(Term Frequency-Inverse Document Frequency) 기반의 벡터화

- 개별 문서에서 자주 나타나는 단어에 높은 가중치
- 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 패널티를 주는 방식으로 값을 부여
- 어떤 문서에서 특정 단어가 자주 나타나면 그 단어는 해당 문서를 특징짓는 중요 단어일 수 있음
- 그러나 그 단어가 다른 문서에서도 자주 나타나는 단어라면
- 해당 단어는 언어 특성상 범용적으로 자주 사용되는 단어일 가능성이 높음
- 예: 여러 뉴스 문서에서 '분쟁', '종교 대립', '유혈 사태' 같은 단어가 자주 나타나는 경우
    - 해당 문서는 지역 분쟁과 관련된 뉴스일 가능성이 높고
        - 해당 단어는 그 문서의 특징을 잘 나타낸다고 할 수 있음
    - 그러나 '많은', '빈번하게', '당연히', '조직', '업무' 등과 같은 단어는
        - 문서의 특징과 관련성이 적지만 보편적으로 많이 사용되기 때문에
        - 문서에 반복적으로 사용될 가능성이 높음
    - 이러한 단어는 단순히 등장하는 횟수에 따라 중요도로 평가받는다면
        - 문서를 특징짓기 어려움
        - 따라서 모든 문서에서 반복적으로 자주 발생하는 단어에 대해서는
        - 패널티를 부여하는 방식으로 단어에 대한 가중치의 균형을 맞춤
    - 문서마다 텍스트가 길고 문서의 개수가 많은 경우
        - 카운드 방식보다는 TF-IDF 방식을 사용하는 것이 더 좋은 예측 성능을 보장할 수 있음

코사인 유사도 계산

---

- 벡터와 벡터 간의 코사인 각도를 이용하여 유사도 산정
- 즉, 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것
![[추천시스템 (1).png]]


- 두 벡터의 방향이
    - 완전히 동일한 경우 : 1
    - 90°: 0 (상관관계 없음)
    - 180°: -1 (완전 반대)

---

- 피처 벡터 행렬은 음수값이 없으므로
    - 코사인 유사도는 음수가 되지 않고
    - 0~1 사이의 값으로 1에 가까울수록 유사도가 높다고 판단

코사인 각도 계산 - cosine_similarity()
![[추천시스템 (37) 1.png]]

![[추천시스템 (2).png]]
![Untitled](Untitled%201.png)

### from sklearn.metrics.pairwise import cosine_similarity

코사인 유사도 구하기위한 패키지

패키지 사용 안한다면

```python
Numpy를 사용해서 코사인 유사도를 계산하는 함수 구현

import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
return dot(A, B)/(norm(A)*norm(B))

각 문서 벡터간의 코사인 유사도 구하기

doc1 = np.array(df)[0]
doc2 = np.array(df)[1]
doc3 = np.array([2,0,2,2])

print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))
print('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))
print('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))
```

![[추천시스템 (38).png]]



![[추천시스템 (3) 1.png]]
# apply 함수

특정한 column의 값들을 일괄적으로 변경하기를 원할 수 있음

apply(lambda)를 활용하는 방법

lambda는 def와 같은 함수와 동일한 기능을 합니다.

def함수(변수):return 결과값처럼 길고 복잡하게 할 필요가 없습니다.

`apply(lambda 입력값 : 결과값)` 순으로 입력

A,B열 데이터만 제곱값으로 변환하는 코드

`df.apply(**lambda** x:np.square(x) if [x.name](http://x.name/) in ['A','B'] else x)`

## **피처 벡터화 함수(CountVectorizer)**

`from sklearn.feature_extraction.text import CountVectorizer`활용

- CountVectorizer
    
    : 단어들의 카운트(**출현 빈도(frequency)**)로 여러 문서들을 벡터화
    카운트 행렬, 단어 문서 행렬 (Term-Document Matrix, TDM))
    모두 소문자로 변환시키기 때문에 me 와 Me 는 모두 같은 특성이 된다.
    
- ***CountVectorizer 매개변수 설명***
	
	[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
	
	ngram_range : n-그램 범위로 단어를 몇 개로 토큰화 할지를 의미합니다.
	  n-그램 범위
	  ngram_range=(1, 1)
	  ngram_range=(2, 2)
	  ngram_range=(1, 2)
	  n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. 모노그램(1-그램)은 토큰 하나만 단어로 사용하며 바이그램(2-그램)은 두 개의 연결된 토큰을 하나의 단어로 사용한다.
	
	max_df : 문서에서 등장하는 최대 빈도수를 의미합니다.
	
	None.
	  정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1
	  최대 몇개 문서에 걸쳐 포함된 단어를 단어 꾸러미에 담아 사용할 것인지
	  단어장에 포함되기 위한 최대 빈도
	  빈도수  
	  max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. 토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시한다. 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다.
	  vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)
	  vect.vocabulary_, vect.stop_words_
	  vect.transform(corpus).toarray().sum(axis=0)
	
	min_df : 문서에서 등장하는 최소 빈도수를 의미합니다.
	
	**in**
	
	range [0.0, 1.0]
	
	None.
	  정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1
	  최소 몇개 문서에 걸쳐 포함된 단어를 단어 꾸러미에 담아 사용할 것인지
	  단어장에 포함되기 위한 최소 빈도
	
	analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수
	  단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램
	  토큰: analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택할 수 있다.
	  vectorizer = CountVectorizer(analyzer="char").fit(corpus)
	  vectorizer.vocabulary_
	
	token_pattern : string
	  토큰 정의용 정규 표현식
	  vect = CountVectorizer(token_pattern="t\w+").fit(corpus)
	  vect.vocabulary_
	
	tokenizer : 함수 또는 None (디폴트)
	  토큰 생성 함수 .
        
    

---

스포티파이 사용되는 알고리즘
![[추천시스템 (4).png]]
![Untitled](Untitled%203.png)

# 실습 - 영화평점을 이용한 콘텐츠기반 필터링 추천

## 1. 데이터 셋 준비

- 영화제목, 장르, 평점, 평점수, popul![[추천시스템 (5).png]]arity, keywords, 영화개요 컬럼
![[추천시스템 (6).png]]
![Untitled](Untitled%204.png)

- 장르 컬럼 데이터 타입 확인
- 장르 컬럼의 값 type 확인 - **str**
문자열로 되어있으므로 파이썬 객체로 변환해주는 
`Series 객체의 apply() 함수에 literal_eval()` 함수를 적용하여 문자열을 객체로 변환
    ![[추천시스템 (7).png]]![[추천시스템 (8).png]]
    ![Untitled](Untitled%205.png)
    
- 다시 장르컬럼의 값 type 확인 - **list**

- 따로 분리되어 있는 영화의 장르 
**name만 추출**해서 하나의 list로 합치기
- for문 사용![[추천시스템 (9).png]]

![Untitled](Untitled%206.png)

- apply(lambda)함수를 이용![[추천시스템 (10) 1.png]]

![Untitled](Untitled%207.png)

- 총 장르가 얼마나 되는지 확인하기 위해 모든 샘플의 장르를 하나의
 list에 결합![[추천시스템 (11).png]]
	
	![Untitled](Untitled%208.png)
	
- set함수![[추천시스템 (12).png]]
(집합을 만드는 함수로 중복데이터 자동 제거)

![Untitled](Untitled%209.png)
    ![[추천시스템 (13).png]]

![Untitled](Untitled%2010.png)
![[추천시스템 (15).png]]
## 함수처리

![[추천시스템 (16).png]]

![[추천시스템 (17).png]]

![[추천시스템 (18).png]]

![[추천시스템 (19).png]]

![[추천시스템 (20).png]]

![[추천시스템 (21).png]]

![[추천시스템 (22).png]]

![[추천시스템 (23).png]]

![[추천시스템 (24).png]]

![[추천시스템 (25).png]]

![[추천시스템 (26).png]]

![[추천시스템 (27).png]]

![[추천시스템 (28).png]]

![[추천시스템 (29).png]]

![[추천시스템 (30) 1.png]]

![[추천시스템 (31) 1.png]]

![[추천시스템 (32) 1.png]]

![[추천시스템 (33) 1.png]]

![[추천시스템 (34) 1.png]]

![[추천시스템 (35) 1.png]]

![[추천시스템 (36) 1.png]]

![[추천시스템 (1) 1.png]]



1. 필요 파라미터

![Untitled](Untitled%2011.png)

- 데이터프레임,
- 레코드별 장르 코사인 유사도 인덱스(유사도가 높은 순으로 정렬된),
- 영화제목,
- 추천할 영화 건수

1. *해당 영화와 유사한 영화를 추천받고 싶음*. 해당 영화에 대한 df를 추출
    
    ![Untitled](Untitled%2012.png)
    
    ![Untitled](Untitled%2013.png)
    
2. 해당 영화의 index.values 값을 ndarray로 저장
    
    ![Untitled](Untitled%2014.png)
    
3. *(유사도가 높은 순으로 정렬된)* 장르 코사인 유사도 인덱스에서 
해당 영화 index를 추출하고, 추천받고싶은 영화건수의 인덱스를 추출
    
    ![Untitled](Untitled%2015.png)
    
    —> [title_index, :(10)] → title_index는 행번호, :(10)은 컬럼범위
    
    - **해당 코사인 유사도 인덱스 추출 알고리즘이 미리 있어야함.**
        
        ## 2. 콘텐츠에 대한 여러 텍스트 정보들을 피처 벡터화
        
        ### 장르 문자열의 count기반 피처 벡터화 진행
        
        ### • from sklearn.feature_extraction.text import CountVectorizer 활용
        
        **`from sklearn.feature_extraction.text import CountVectorizer`**
        
        - 리스트로 구성되어 있는 장르컬럼을 문자열로 변경한뒤
        Count 기반 피처 벡터화 변환 (행렬로 변환)
            
            ![Untitled](Untitled%2016.png)
            
        
        - 현재 이러한 장르데이터를 CountVectirizer를 적용하기 위해 
        공백 문자로 word단위가 구분되는 문자열로 변환
            - **join( ) 메소드** 사용해 리스트의 각 원소들을 공백으로 결합해서 문자열로 변환
            
            ![apply코드를 이해하기 위한 for문](Untitled%2017.png)
            
            apply코드를 이해하기 위한 for문
            
            - **apply(lambda) 이용**
        
        ![Untitled](Untitled%2018.png)
        
        → **ngram_range 단어 최대 2개까지 이용해서 피처를 생성,** 
              단어 1개 피처 단어 2개를 조합한 피처 생성
              # min_df : 문서에서 등장하는 최소 빈도수
              # ngram_range : n-그램 범위로 단어를 몇개로 토큰화 할지
        
        ![Untitled](Untitled%2019.png)
        
        **위에서 공백처리된 'genres_literal’컬럼을 countVectorizer 벡터화 학습진행**
        
        `벡터화된 장르리스트`
        
        ![Untitled](Untitled%2020.png)
        
        `각 feature의 인덱스들`
        
        ![Untitled](Untitled%2021.png)
        
        **transform으로 장르를 매트릭스로 변환**
        
        - 장르의 용어 매트릭스로 변환
        **원시 텍스트 문서에서 토큰 수를 추출**합니다.
        
        ![Untitled](Untitled%2022.png)
        
        ![Untitled](Untitled%2023.png)
        
        - 결과
            
            **4803개의 레코드**와 **276개의 개별 단어로 피처로 구성된 피처 벡터 행렬 생성**됨 
            ngram_range : 단어의 묶음
            ngram_range = (1, 1) : 1개 단어 묶음 ('home') (토큰 하나만 단어로 사용)
            ngram_range = (1, 2) : 1개 또는 2개 단어 묶음 ('go home') 
                                      (두 개의 연결된 토큰을 단어로 사용)
            min_df : 단어장에 포함되기 위한 최소 빈도. 해당 값보다 빈도수가 낮은 단어는 무시
            
        
        ![Untitled](Untitled%2024.png)
        
        ## 3. **코사인 유사도로 컨텐츠별 유사도 계산**
        
        영화별, 장르에 따라 
        유사도가 어떻게되는지 ?
        
        cosine_simliarity를 “genre_mat”을 2번 넣어서 유사도를 구했는데, 4803개의 영화별 장르토큰수와 4803개의 영화별 장르토큰수를 구한거라 해당 영화의 각 장르별 
        
        ![Untitled](Untitled%2025.png)
        
        ![Untitled](Untitled%2026.png)
        
        → 4803개의 영화
        
        ### **genre_sim**
        
        - movies_df의 genre_literal(genre 장르 join) 칼럼을 
        피처 벡터화한 행렬 genre_mat (원시 텍스트의 토큰 수)데이터의 
        행별 유사도 정보를 가지고 있음
        - 즉, movies_df 데이터프레임의 행렬 장르 유사도 값을 가지고 있음
        - movies_df를 장르 기준으로 컨텐츠 기반 필터링을 수행하려면
        - movies_df의 개별 레코드에 대해서
            - 가장 장르 유사도가 높은 순으로 다른 레코드를 추출해야 하는데
            - 이때 생성된 genre_sim(장르별 유사도) 객체 이용
        - genre_sim 객체의 기준 행렬로 비교 대상이 되는 행의
            - 유사도 값이 높은 순으로 정렬된 행렬의 위치 인텍스 값을 추출하면 됨
        
        **주의!!**
        
        - 값이 높은 순으로 정렬된 비교 대상 행의 유사도 값이 아니라
            - 비교 대상 행의 위치 인덱스임에 주의
            - 높은 순 (내림차순 정렬)
            - 정렬 후 위치 인덱스를 반환하는 함수가 필요
                - **argsort() :** 배열을 오름차순 정렬하기 위한 index 값을 배열로 반환
        
        ### **genre_sim 배열 변수에 대해 각 행별로 내림차순 정렬 후 인덱스를 반환**
        
        ![Untitled](Untitled%2027.png)
        
        ![Untitled](Untitled%2028.png)
        
    
    ![Untitled](Untitled%2029.png)
    
    ***→ 가중평점 적용할거면 영화건수 *2로 후보군을 2배로 늘리기***
    
4. shape이 (1,10) 2차원배열 형태여서 **reshape(-1)로 1차원배열**로 반환

![Untitled](Untitled%2030.png)

1. 기준 영화 index는 제외 (파라미터로 받은 영화, 자기는 제외)

![Untitled](Untitled%2031.png)

1. 기존 df에 iloc로 위 인덱스로 추출을 하면
해당 영화와 유사도가 높은 영화를 추천받을 수 있음

![Untitled](Untitled%2032.png)

→ ***가중평점을 적용해서 정렬할꺼면 top_n의 2배 후보군에서 weighted_vote가 높은순으로 추출***

![Untitled](Untitled%2033.png)

- **평점에 따라 필터링해서 최종 추천 - 가중평점 적용**
    - 0~10점 만점 범위에서 여러 관걕이 평가한 평점을 평균한 것인데
    - 1~2명 소수의 관객이 특정 영화에 만점이나 매우 높은 평점을 부여해 왜곡된 데이터 포함
    - sort_values() 이용 오름차순 정렬해서 확인
        
        ![Untitled](Untitled%2034.png)
        
        ### 가중평점 계산
        
        **평가 횟수에 대한 가중치가 부여된 평점(Weighted Rating) 계산
        가중 평점(Weighted Rating) = (v/(v+m)) * R + (m/(v+m)) * C**
        
        ■ v: 개별 영화에 평점을 투표한 횟수 (vote_count)
        ■ m: 평점을 부여하기 위한 최소 투표 횟수
        ■ R: 개별 영화에 대한 평균 평점 (vote_average)
        ■ C: 전체 영화에 대한 평균 평점
        
        ![Untitled](Untitled%2035.png)
        
        → **가중평점 계산한 새로운 컬럼 생성**
        
        ![Untitled](Untitled%2036.png)
        
        ![Untitled](Untitled%2037.png)
        
        ### 새롭게 정의된 평점 기준에 따라 영화 추천
        
        - 장르 유사성이 높은 영화를 top_n의 2배수만큼 후보군으로 선정한 뒤
        - weighted_vote 칼럼 값이 높은 순으로
        - top_n만큼 추출하는 방식

- 추천시스템 - 콘텐츠기반필터링
    
    영화에 대한 정보 데이터가 있음
    
    - 영화제목, 장르, 평점, 평점수, 영화개요 등등 컬럼(4803행)
    
    장르를 통해 사용자가 본 장르와 유사한 콘텐츠를 추천해줄거임
    

### 내가 본 영화를 입력하면 이와 유사한 영화 10개를 보여준다.

보여주는 리스트는 1.유사도 2. 가중평점 기준으로 보여줌

추천시스템이 처리될때 필요한건

1. (가중평점, 장르컬럼 데이터형태 변환 처리된)영화에대한 데이터
2. 위 처리된 영화데이터를 이용하여 카운트벡터라이저 진행
장르 컬럼을 ngram피처를 생성할수있게 가볍게 전처리를 하고
토큰화를 진행한 후 벡터화 학습진행
매트릭스로 변환하여 피처벡터행렬로 df로 만듦
3. 피처벡터행렬 df를 사이킷런의 코사인시밀리어티로 유사도 계산
각 영화별 장르 유사도를 가진 데이터를 생성함(4803*4803)
이 데이터를 argsort함수로 유사도가 높은순으로 정렬하여 저장
4. 영화평점, 평점투표수를 이용하여 새로운 가중평점 생성
위 가중평점을 함수로 구하여 새로운 컬럼으로 추가
5. 위 데이터처리가 모두 끝나면 함수 만들수잇음

필요파라미터를 입력하면
- 데이터프레임,
- 레코드별 장르 코사인 유사도 인덱스(유사도가 높은 순으로 정렬된),
- 영화제목,
- 추천할 영화 건수

- 영화제목을 데이터프레임에서 추출해서
- 해당 영화 인덱스를 저장하고
- 코사인유사도 데이터에서 해당 인덱스와 유사도가 높은 인덱스를
추천할 영화건수 *2 배 후보군만큼 추출
- 인덱스 쉐입이 2차원배열이라 1차원으로 리쉐입해주고
- 기준 영화는 제외해주고 리턴

리턴형태는 데이터프레임에 iloc를 저장된 유사도 인덱스를 지정하고,
sort_values로 가중평점 컬럼이 높은순으로 추출함 (추천받 영화 건수만큼)

노래에 대한 정보 데이터.

- 노래제목 or 플레이리스트 제목
- 노래 or 플레이리스트 url
- 해당 리스트를 판단할수있는 (평점같은) ….
    - 멜론은 좋아요하트수(4803), 유튜브는 워낙 좋아요수가 (1.1만) 이라… 
    가중평점과 같은 새로운 평가방식이 필요할거같음.
    - 유튜브 - 좋아요와 조회수
    - 멜론 -
- 감정 8가지를 각 노래리스트에도 부여를 해준다.
    - 즉, 장르와 같은 카테고리를 만들어야함.
    - 실습데이터에 장르는 (20)개였고, 멀티라벨로 들어가서 단어토큰화가 가능했음.
    - 감정장르를 지정할거면 8가지감정을 2~3개로 또 분류하는 것이 어떤지
    - ex) 기쁨 - 신남, 행복, 즐거움 / 분노 - 화남, 등등등,, 
    아니면 감정 소분류로 지정한걸로 활용해보면 어떤지

유튜브

영상제목 **여행갈 때 듣기좋은 텐션UP 노래모음🛫 | 재업로드 | PLAYLIST | 가사포함**

영상 url

태그 [`#여행갈때듣기좋은노래](https://www.youtube.com/hashtag/%EC%97%AC%ED%96%89%EA%B0%88%EB%95%8C%EB%93%A3%EA%B8%B0%EC%A2%8B%EC%9D%80%EB%85%B8%EB%9E%98) [#여행노래](https://www.youtube.com/hashtag/%EC%97%AC%ED%96%89%EB%85%B8%EB%9E%98) [#신나는노래](https://www.youtube.com/hashtag/%EC%8B%A0%EB%82%98%EB%8A%94%EB%85%B8%EB%9E%98)`

태그로 들어가면 관련 노래 리스트가 나옴

→ *필요하다면 각 플레이리스트 곡 정보가 더보기란에  있음*

댓글 

`기분 안 좋은 엄청 낮은 텐션도 지금 막 여행갈것처럼 신나게 텐션을 높여주는 playlist 예요!! 몇주 뒤면 롯데월드로 체험학습 가는데 엄청 즐겁게 들으면서 텐션 높게 해주실것 같아요! 진짜 이런 노래만 모아주셔서 너무 감사해요.. 텐션 확 높여주셔서 정말로 감사합니다! 
 그리고 이 댓글 보신분들도 오늘 하루 활기차고 즐겁게 보내세요!ヽ(´▽`)/`

문장 단어토큰화하여 감정단어를 추출해서 해당 감정의 점수부여?

아니면 영상제목의 문장단어와 연관성이 높은 댓글이 많다면 추천도 up

1. 데이터수집 크롤링 계획

감정 정해져잇음, 그 감정으로 분류 , 곡의 태그 이용 

태그를 감정으로 / 플레이리스트 추천 - TF-IDF 만들어야함

태그별로 감정 8가지로 분류

[파이썬을 이용한 유튜브 채널 크롤링, csv파일로 만들기](https://jh-0323.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9C%A0%ED%8A%9C%EB%B8%8C-%EC%B1%84%EB%84%90-%ED%81%AC%EB%A1%A4%EB%A7%81-csv%ED%8C%8C%EC%9D%BC%EB%A1%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0)